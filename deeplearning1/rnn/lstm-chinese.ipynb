{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/PeiYingchi/Documents/fastai-notes/deeplearning1/rnn/text\n"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "import os\n",
    "import sys\n",
    "import codecs\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "start_token = 'G'\n",
    "end_token = 'E'\n",
    "\n",
    "data_dir = '~/Documents/fastai-notes/deeplearning1/rnn/text/'\n",
    "%cd $data_dir\n",
    "filename = 'gucheng_compiled.txt'\n",
    "\n",
    "poems = []\n",
    "with open(filename, 'r', encoding='utf-8') as f:\n",
    "    for line in f.readlines():\n",
    "        try:\n",
    "            title, content = line.strip().split(':')\n",
    "            content = content.replace(' ', '')\n",
    "            if len(content) <= 10:\n",
    "                continue\n",
    "            content = start_token + content + end_token\n",
    "            poems.append(content)\n",
    "        except ValueError as e:\n",
    "            pass\n",
    "poems = sorted(poems, key=lambda line: len(line))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['G连睡梦的路，都难以到达E',\n",
       " 'G剪下一页页生活的片断；E',\n",
       " 'G像一枚，毫不掩饰的果实E',\n",
       " 'G他似笑非笑，说，你早。E',\n",
       " 'G没有回答，表情似笑非笑E',\n",
       " 'G飞来又飞去，飞高又飞低E',\n",
       " 'G因为有问题，因为有秘密E',\n",
       " 'G一阵阵快乐在尾巴上跳荡E',\n",
       " 'G再没有海岸，再没有灯火E',\n",
       " 'G干燥的红星星，全都脱落E']"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "poems[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_words = []\n",
    "for poem in poems:\n",
    "    all_words += [word for word in poem]\n",
    "# 这里根据包含了每个字对应的频率\n",
    "counter = collections.Counter(all_words)\n",
    "count_pairs = sorted(counter.items(), key=lambda x: -x[1])\n",
    "words, _ = zip(*count_pairs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 取前多少个常用字\n",
    "words = words[:len(words)] + (' ',)\n",
    "# 每个字映射为一个数字ID\n",
    "word_int_map = dict(zip(words, range(len(words))))\n",
    "word2idfunc = lambda word:  word_int_map.get(word,len(words))\n",
    "poems_vector = [list(map(word2idfunc, poem)) for poem in poems]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('，', '的', '在', '一', 'G')"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "#batch-wise padding:do padding to the same size(sequence length) of each batch\n",
    "batch_size = 16\n",
    "n_batch = (len(poems_vector)-1) // batch_size\n",
    "X_data,Y_data = [],[]\n",
    "for i in range(n_batch):\n",
    "    cur_vecs = poems_vector[i*batch_size:(i+1)*batch_size]\n",
    "    current_batch_max_length = max(map(len,cur_vecs))\n",
    "    batch_matrix = np.full((batch_size,current_batch_max_length),word2idfunc(\" \"),np.int32)\n",
    "    for j in range(batch_size):\n",
    "        batch_matrix[j,:len(cur_vecs[j])] = cur_vecs[j]\n",
    "    x = batch_matrix\n",
    "    X_data.append(x)\n",
    "    y = np.copy(x)\n",
    "    y[:,:-1] = x[:,1:]\n",
    "    # print x\n",
    "    # print y\n",
    "    Y_data.append(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "#build rnn\n",
    "vocab_size = len(words)+1\n",
    "#input_size:(batch_size,feature_length)\n",
    "input_sequences = tf.placeholder(tf.int32,shape=[batch_size,None])\n",
    "output_sequences = tf.placeholder(tf.int32,shape=[batch_size,None])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_rnn(hidden_units=128, layers=2):\n",
    "    # embedding\n",
    "    with tf.variable_scope(\"embedding\", reuse=True):\n",
    "        embedding = tf.get_variable(\"embedding\", [vocab_size, hidden_units], dtype=tf.float32)\n",
    "        input = tf.nn.embedding_lookup(embedding, input_sequences)\n",
    "        \n",
    "    basic_cell = tf.nn.rnn_cell.BasicLSTMCell(hidden_units, state_is_tuple=True)\n",
    "    stack_cell = tf.nn.rnn_cell.MultiRNNCell([basic_cell]*layers)\n",
    "    _initial_state = stack_cell.zero_state(batch_size, tf.float32)\n",
    "    outputs, state = tf.nn.dynamic_rnn(stack_cell, input, initial_state=_initial_state, dtype=tf.float32)\n",
    "    outputs = tf.reshape(outputs, [-1, hidden_units])\n",
    "    \n",
    "    with tf.variable_scope('softmax'):\n",
    "        softmax_w = tf.get_variable('softmax_w', [hidden_units, vocab_size])\n",
    "        softmax_b = tf.get_variable('softmax_b', [vocab_size])\n",
    "        logits = tf.matmul(outputs, softmax_w)+softmax_b\n",
    "        \n",
    "    probs = tf.nn.softmax(logits)\n",
    "    return logits, probs, stack_cell, _initial_state, state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(reload=True):\n",
    "    logits, probs,_,_,_ = build_rnn()\n",
    "\n",
    "    targets = tf.reshape(output_sequences,[-1])\n",
    "\n",
    "    loss = tf.nn.seq2seq.sequence_loss_by_example([logits], [targets], \n",
    "        [tf.ones_like(targets, dtype=tf.float32)],len(words))\n",
    "    cost = tf.reduce_mean(loss)\n",
    "\n",
    "    learning_rate = tf.Variable(0.002, trainable=False)\n",
    "    tvars = tf.trainable_variables()\n",
    "    grads, _ = tf.clip_by_global_norm(tf.gradients(cost, tvars), 5)\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "    train_op = optimizer.apply_gradients(zip(grads, tvars))\n",
    "\n",
    "    global_step = 0\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.initialize_all_variables())\n",
    "        saver = tf.train.Saver(write_version=tf.train.SaverDef.V2)\n",
    "\n",
    "        if reload:\n",
    "            module_file = tf.train.latest_checkpoint('.')\n",
    "            sess = saver.restore(module_file)\n",
    "            print(\"reload sess\")\n",
    "\n",
    "        for epoch in range(50):\n",
    "            print(\"learning_rate decrease\")\n",
    "            if global_step%80==0:\n",
    "                sess.run(tf.assign(learning_rate, 0.002 * (0.97 ** epoch)))\n",
    "            epoch_steps =  len(zip(X_data,Y_data))\n",
    "            for step,(x,y) in enumerate(zip(X_data,Y_data)):\n",
    "                global_step = epoch * epoch_steps + step\n",
    "                _, los  = sess.run([train_op, cost], feed_dict={\n",
    "                    input_sequences:x,\n",
    "                    output_sequences:y,\n",
    "                    })\n",
    "                print(\"epoch:%d steps:%d/%d loss:%3f\" % (epoch,step,epoch_steps,los))\n",
    "                if global_step%100==0:\n",
    "                    print(\"save model\")\n",
    "                    saver.save(sess,\"peotry\",global_step=epoch)\n",
    "\n",
    "\n",
    "def write_poem():\n",
    "    def to_word(weights):\n",
    "        t = np.cumsum(weights)\n",
    "        s = np.sum(weights)\n",
    "        sample = int(np.searchsorted(t, np.random.rand(1)*s))\n",
    "        print(\"sample:\",sample)\n",
    "        print(\"len Words:\",len(words))\n",
    "        return words[sample]\n",
    "\n",
    "    logits, probs,stack_cell, _initial_state, last_state = build_rnn()\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.initialize_all_variables())\n",
    "        saver = tf.train.Saver(write_version=tf.train.SaverDef.V2)\n",
    "        module_file = tf.train.latest_checkpoint('.')\n",
    "        print(\"load:\",module_file)\n",
    "        saver.restore(sess,module_file)\n",
    "\n",
    "        _state = sess.run(stack_cell.zero_state(1,dtype=tf.float32))\n",
    "\n",
    "        x = np.array([[word2idfunc('[')]])\n",
    "\n",
    "        prob_, _state = sess.run([probs,last_state],feed_dict={input_sequences:x,_initial_state:_state})\n",
    "\n",
    "        word = to_word(prob_)\n",
    "\n",
    "        poem = ''\n",
    "\n",
    "        import time\n",
    "        while word != ']':\n",
    "            poem += word\n",
    "            x = np.array([[word2idfunc(word)]])\n",
    "            [probs_, _state] = sess.run([probs, last_state], feed_dict={input_sequences: x, _initial_state: _state})\n",
    "            word = to_word(probs_)\n",
    "            # time.sleep(1)\n",
    "\n",
    "    return poem\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/PeiYingchi/anaconda/lib/python3.5/site-packages/tensorflow/python/util/tf_should_use.py:170: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n",
      "load: None\n",
      "INFO:tensorflow:Restoring parameters from None\n"
     ]
    },
    {
     "ename": "SystemError",
     "evalue": "<built-in function TF_Run> returned a result with an error set",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;31mTypeError\u001b[0m: expected bytes, NoneType found",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mSystemError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-59-f1ccb6ced765>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwrite_poem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-58-421cdf6d33fa>\u001b[0m in \u001b[0;36mwrite_poem\u001b[0;34m()\u001b[0m\n\u001b[1;32m     56\u001b[0m         \u001b[0mmodule_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlatest_checkpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"load:\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmodule_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m         \u001b[0msaver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrestore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmodule_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m         \u001b[0m_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstack_cell\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/PeiYingchi/anaconda/lib/python3.5/site-packages/tensorflow/python/training/saver.py\u001b[0m in \u001b[0;36mrestore\u001b[0;34m(self, sess, save_path)\u001b[0m\n\u001b[1;32m   1546\u001b[0m     \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Restoring parameters from %s\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1547\u001b[0m     sess.run(self.saver_def.restore_op_name,\n\u001b[0;32m-> 1548\u001b[0;31m              {self.saver_def.filename_tensor_name: save_path})\n\u001b[0m\u001b[1;32m   1549\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1550\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/PeiYingchi/anaconda/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    787\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    788\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 789\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    790\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    791\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/PeiYingchi/anaconda/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    995\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    996\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 997\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    998\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    999\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/PeiYingchi/anaconda/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1130\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1131\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1132\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1133\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1134\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m/Users/PeiYingchi/anaconda/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1137\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1138\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1139\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1140\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1141\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/PeiYingchi/anaconda/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1119\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[1;32m   1120\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1121\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m   1122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1123\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mSystemError\u001b[0m: <built-in function TF_Run> returned a result with an error set"
     ]
    }
   ],
   "source": [
    "print(write_poem())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
