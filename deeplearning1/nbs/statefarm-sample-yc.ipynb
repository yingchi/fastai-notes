{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Enter State Farm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want some tutorial, follow the video from Lesson 48:50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING (theano.sandbox.cuda): The cuda backend is deprecated and will be removed in the next release (v0.10).  Please switch to the gpuarray backend. You can get more information about how to switch at this URL:\n",
      " https://github.com/Theano/Theano/wiki/Converting-to-the-new-gpu-back-end%28gpuarray%29\n",
      "\n",
      "Using gpu device 0: Tesla K80 (CNMeM is disabled, cuDNN 5103)\n",
      "WARNING (theano.sandbox.cuda): The cuda backend is deprecated and will be removed in the next release (v0.10).  Please switch to the gpuarray backend. You can get more information about how to switch at this URL:\n",
      " https://github.com/Theano/Theano/wiki/Converting-to-the-new-gpu-back-end%28gpuarray%29\n",
      "\n",
      "WARNING (theano.sandbox.cuda): Ignoring call to use(1), GPU number 0 is already in use.\n"
     ]
    }
   ],
   "source": [
    "from theano.sandbox import cuda\n",
    "cuda.use('gpu1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "from __future__ import division, print_function\n",
    "from importlib import reload\n",
    "import utils; reload(utils)\n",
    "from utils import *\n",
    "from IPython.display import FileLink\n",
    "\n",
    "path = 'data/state/sample/'\n",
    "batch_size = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "First, let's get the data ready.\n",
    "\n",
    "Move to the data folder and create a new directory for statefarm:\n",
    "\n",
    "```bash\n",
    "cd ~/fastai-notes/deeplearning1/nbs/data/\n",
    "mkdir state\n",
    "cd state\n",
    "```\n",
    "\n",
    "Download the data from kaggle. Note: don't forget to accept the competition rule.\n",
    "```bash\n",
    "kg download -u yingchi.pei@gmail.com -p FL199473/kag -c state-farm-distracted-driver-detection\n",
    "```\n",
    "Unzip the downloaded data:\n",
    "```bash\n",
    "unzip -q imgs.zip\n",
    "unzip -q driver_imgs_list.csv.zip\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the data ready"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "#### Create the validation set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Count the number of files in the current directory: `find . -type f | wc -l`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "In each category's folder inside /train, there are around 2.2K images. So let's take 200 from each category to the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/fastai-notes/deeplearning1/nbs/data/state\n",
      "mkdir: cannot create directory ‘valid’: File exists\n",
      "mkdir: cannot create directory ‘results’: File exists\n",
      "/home/ubuntu/fastai-notes/deeplearning1/nbs/data/state/train\n"
     ]
    }
   ],
   "source": [
    "DATA_HOME_DIR='/home/ubuntu/fastai-notes/deeplearning1/nbs/data/state'\n",
    "%cd $DATA_HOME_DIR\n",
    "%mkdir valid\n",
    "%mkdir results\n",
    "%cd train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "for d in glob('c?'): os.mkdir('../valid/'+d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "g = glob('c?/*.jpg')\n",
    "shuf = np.random.permutation(g)\n",
    "for i in range(2000): os.rename(shuf[i], DATA_HOME_DIR+'/valid/'+shuf[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "#### Create the sample set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/fastai-notes/deeplearning1/nbs/data/state\n"
     ]
    }
   ],
   "source": [
    "%cd $DATA_HOME_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "%mkdir -p sample/train\n",
    "%mkdir -p sample/valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/fastai-notes/deeplearning1/nbs/data/state/train\n"
     ]
    }
   ],
   "source": [
    "%cd train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from shutil import copyfile\n",
    "\n",
    "for d in glob('c?'): \n",
    "    os.mkdir('../sample/train/'+d)\n",
    "    os.mkdir('../sample/valid/'+d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "g = glob('c?/*.jpg')\n",
    "shuf = np.random.permutation(g)\n",
    "for i in range(1500): copyfile(shuf[i], '../sample/train/'+shuf[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/fastai-notes/deeplearning1/nbs/data/state/valid\n"
     ]
    }
   ],
   "source": [
    "%cd ../valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "g = glob('c?/*.jpg')\n",
    "shuf = np.random.permutation(g)\n",
    "for i in range(1000): copyfile(shuf[i], '../sample/valid/'+shuf[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false,
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/fastai-notes/deeplearning1/nbs\n"
     ]
    }
   ],
   "source": [
    "%cd ../../..\n",
    "%mkdir data/state/sample/test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1500 images belonging to 10 classes.\n",
      "Found 1000 images belonging to 10 classes.\n"
     ]
    }
   ],
   "source": [
    "batches = get_batches(path+'train', batch_size=batch_size)\n",
    "val_batches = get_batches(path+'valid', batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```py\n",
    "def get_batches(dirname, gen=image.ImageDataGenerator(), shuffle=True, \n",
    "                batch_size=4, class_mode='categorical', target_size=(224,224)):\n",
    "    return gen.flow_from_directory(dirname, target_size=target_size, \n",
    "           class_mode=class_mode, shuffle=shuffle, batch_size=batch_size)\n",
    "```\n",
    "\n",
    "From keras:\n",
    "```py\n",
    "fit_generator(self, generator, steps_per_epoch, epochs=1, verbose=1, callbacks=None, validation_data=None, validation_steps=None, class_weight=None, max_queue_size=10, workers=1, use_multiprocessing=False, initial_epoch=0)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "??get_classes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1500 images belonging to 10 classes.\n",
      "Found 1000 images belonging to 10 classes.\n",
      "Found 0 images belonging to 0 classes.\n"
     ]
    }
   ],
   "source": [
    "(val_classes, trn_classes, val_labels, trn_labels, val_filenames, filenames,\n",
    "    test_filename) = get_classes(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Linear model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we try the simplest model and use default parameter. \n",
    "\n",
    "Note the trick of making the first layer a batchnorm layer - so that we don't have to worry about normalizing the input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = Sequential([\n",
    "        BatchNormalization(axis=1, input_shape=(3, 224, 224)),\n",
    "        Flatten(),\n",
    "        Dense(10, activation='softmax')\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "1500/1500 [==============================] - 32s - loss: 13.6292 - acc: 0.1500 - val_loss: 13.3279 - val_acc: 0.1700\n",
      "Epoch 2/2\n",
      "1500/1500 [==============================] - 24s - loss: 14.3084 - acc: 0.1107 - val_loss: 13.7446 - val_acc: 0.1460\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fc0780cada0>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(Adam(), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model.fit_generator(batches, batches.nb_sample, nb_epoch=2,\n",
    "                   validation_data=val_batches, nb_val_samples=val_batches.nb_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ya, this model training is going nowhere...\n",
    "Let's check the number of parameters in our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "batchnormalization_1 (BatchNorma (None, 3, 224, 224)   12          batchnormalization_input_1[0][0] \n",
      "____________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)              (None, 150528)        0           batchnormalization_1[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "dense_1 (Dense)                  (None, 10)            1505290     flatten_1[0][0]                  \n",
      "====================================================================================================\n",
      "Total params: 1,505,302\n",
      "Trainable params: 1,505,296\n",
      "Non-trainable params: 6\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Over 1.5 million parameters - that should be enough. Incidentally, it's worth checking you understand why this is the number of parameters in this layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1505280"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "10*3*224*224"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we have a simple model with no regularization and plenty of parameters, it seems most likely that our learning rate is too high. Perhaps it is jumping to a solution where it predicts 1 or 2 classes wigh high confidence, so that it can give a 0 prediction to as many classes as possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.],\n",
       "       [ 1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.]], dtype=float32)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.round(model.predict_generator(batches, batches.N)[:20], 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our hypothesis was correct. It's nearly always predicting class 1 or 9. \n",
    "\n",
    "From keras documentation, the default params for Adam() is:\n",
    "`Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)`\n",
    "\n",
    "So let's try a lower learning rate:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n",
      "1500/1500 [==============================] - 32s - loss: 4.6020 - acc: 0.2793 - val_loss: 8.0088 - val_acc: 0.2700\n",
      "Epoch 2/4\n",
      "1500/1500 [==============================] - 24s - loss: 2.3559 - acc: 0.6407 - val_loss: 3.1900 - val_acc: 0.5210\n",
      "Epoch 3/4\n",
      "1500/1500 [==============================] - 24s - loss: 1.8422 - acc: 0.7927 - val_loss: 2.2388 - val_acc: 0.7370\n",
      "Epoch 4/4\n",
      "1500/1500 [==============================] - 24s - loss: 1.6826 - acc: 0.8467 - val_loss: 2.4954 - val_acc: 0.6940\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fc070d6c668>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Sequential([\n",
    "        BatchNormalization(axis=1, input_shape=(3, 224, 224)),\n",
    "        Flatten(),\n",
    "        Dense(10, activation='softmax')\n",
    "    ])\n",
    "model.compile(Adam(lr=0.0001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model.fit_generator(batches, batches.nb_sample, nb_epoch=4, validation_data=val_batches, \n",
    "                 nb_val_samples=val_batches.nb_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're stabilizing at validation accuracy of 0.7. Much better than a random guess!\n",
    "\n",
    "Before moving on, let's check that our validation set on the sample is large enough that it gives consistent results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1000 images belonging to 10 classes.\n"
     ]
    }
   ],
   "source": [
    "rnd_batches = get_batches(path+'valid', batch_size=batch_size*2, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 2.58,  0.68],\n",
       "       [ 2.38,  0.69],\n",
       "       [ 2.71,  0.67],\n",
       "       [ 2.46,  0.71],\n",
       "       [ 2.53,  0.68],\n",
       "       [ 2.56,  0.68],\n",
       "       [ 2.49,  0.69],\n",
       "       [ 2.64,  0.67],\n",
       "       [ 2.44,  0.71],\n",
       "       [ 2.63,  0.67]])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Look at 10 randomly batches validation sets, and look at the val_acc\n",
    "val_res = [model.evaluate_generator(rnd_batches, rnd_batches.nb_sample) for i in range(10)]\n",
    "np.round(val_res, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### L2 regularization "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Up to now, the previous models are over-fitting. Note that we can't user dropout since we only have one simple linear layer. Let's try to descrease overfitting by adding l2 regularization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n",
      "1500/1500 [==============================] - 32s - loss: 5.2144 - acc: 0.2973 - val_loss: 9.9192 - val_acc: 0.2370\n",
      "Epoch 2/4\n",
      "1500/1500 [==============================] - 24s - loss: 2.6806 - acc: 0.5960 - val_loss: 4.2026 - val_acc: 0.4470\n",
      "Epoch 3/4\n",
      "1500/1500 [==============================] - 24s - loss: 1.3007 - acc: 0.7213 - val_loss: 1.6341 - val_acc: 0.6700\n",
      "Epoch 4/4\n",
      "1500/1500 [==============================] - 24s - loss: 0.7057 - acc: 0.8607 - val_loss: 0.9395 - val_acc: 0.8110\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f5df4dcf898>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Sequential([\n",
    "        BatchNormalization(axis=1, input_shape=(3, 224, 224)),\n",
    "        Flatten(),\n",
    "        Dense(10, activation='softmax', W_regularizer=l2(0.01))\n",
    "    ])\n",
    "model.compile(Adam(lr=0.0001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model.fit_generator(batches, batches.nb_sample, nb_epoch=4, validation_data=val_batches, \n",
    "                 nb_val_samples=val_batches.nb_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will be a good benchmark for our future models - if we can't beat 80%, then we're not even beating a linear model trained on a sample, so we'll know that's not a good approach."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Single hidden layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "1500/1500 [==============================] - 32s - loss: 2.0024 - acc: 0.3640 - val_loss: 7.2128 - val_acc: 0.1330\n",
      "Epoch 2/2\n",
      "1500/1500 [==============================] - 24s - loss: 1.1389 - acc: 0.6913 - val_loss: 3.0983 - val_acc: 0.2890\n",
      "Epoch 1/5\n",
      "1500/1500 [==============================] - 32s - loss: 0.6864 - acc: 0.8487 - val_loss: 1.3658 - val_acc: 0.5700\n",
      "Epoch 2/5\n",
      "1500/1500 [==============================] - 24s - loss: 0.4720 - acc: 0.9273 - val_loss: 0.8339 - val_acc: 0.7500\n",
      "Epoch 3/5\n",
      "1500/1500 [==============================] - 24s - loss: 0.3135 - acc: 0.9600 - val_loss: 0.6967 - val_acc: 0.8200\n",
      "Epoch 4/5\n",
      "1500/1500 [==============================] - 24s - loss: 0.2304 - acc: 0.9840 - val_loss: 0.5857 - val_acc: 0.8610\n",
      "Epoch 5/5\n",
      "1500/1500 [==============================] - 24s - loss: 0.1706 - acc: 0.9933 - val_loss: 0.5138 - val_acc: 0.9070\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f5decffecc0>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Sequential([\n",
    "        BatchNormalization(axis=1, input_shape=(3, 224, 224)),\n",
    "        Flatten(),\n",
    "        Dense(100, activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        Dense(10, activation='softmax')\n",
    "    ])\n",
    "model.compile(Adam(lr=1e-5), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model.fit_generator(batches, batches.nb_sample, nb_epoch=2, validation_data=val_batches, \n",
    "                 nb_val_samples=val_batches.nb_sample)\n",
    "\n",
    "model.optimizer.lr = 0.01\n",
    "model.fit_generator(batches, batches.nb_sample, nb_epoch=5, validation_data=val_batches, \n",
    "                 nb_val_samples=val_batches.nb_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Single conv layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2 conv layers with max pooling followed by a simple dense network is a good sample CNN to start:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def conv1(batches):\n",
    "    model = Sequential([\n",
    "            BatchNormalization(axis=1, input_shape=(3,224,224)),\n",
    "            Convolution2D(32,3,3, activation='relu'),\n",
    "            BatchNormalization(axis=1),\n",
    "            MaxPooling2D((3,3)),\n",
    "            Convolution2D(64,3,3, activation='relu'),\n",
    "            BatchNormalization(axis=1),\n",
    "            MaxPooling2D((3,3)),\n",
    "            Flatten(),\n",
    "            Dense(200, activation='relu'),\n",
    "            BatchNormalization(),\n",
    "            Dense(10, activation='softmax')\n",
    "        ])\n",
    "    \n",
    "    model.compile(Adam(lr=0.001),  loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    model.fit_generator(batches, batches.nb_sample, nb_epoch=2, validation_data=val_batches, \n",
    "                     nb_val_samples=val_batches.nb_sample)\n",
    "    model.optimizer.lr = 0.01\n",
    "    model.fit_generator(batches, batches.nb_sample, nb_epoch=4, validation_data=val_batches, \n",
    "                     nb_val_samples=val_batches.nb_sample)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "1500/1500 [==============================] - 33s - loss: 1.6943 - acc: 0.5407 - val_loss: 2.2134 - val_acc: 0.3720\n",
      "Epoch 2/2\n",
      "1500/1500 [==============================] - 26s - loss: 0.5735 - acc: 0.8733 - val_loss: 2.1744 - val_acc: 0.2820\n",
      "Epoch 1/4\n",
      "1500/1500 [==============================] - 32s - loss: 0.1649 - acc: 0.9780 - val_loss: 2.3287 - val_acc: 0.2760\n",
      "Epoch 2/4\n",
      "1500/1500 [==============================] - 24s - loss: 0.0618 - acc: 0.9940 - val_loss: 2.4012 - val_acc: 0.2550\n",
      "Epoch 3/4\n",
      "1500/1500 [==============================] - 26s - loss: 0.0257 - acc: 0.9987 - val_loss: 2.6948 - val_acc: 0.2340\n",
      "Epoch 4/4\n",
      "1500/1500 [==============================] - 24s - loss: 0.0122 - acc: 1.0000 - val_loss: 2.4649 - val_acc: 0.2940\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.models.Sequential at 0x7f5dec982128>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv1(batches)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training set here is very rapidly reaching a very high accuracy. So if we could regularize this, perhaps we could get a reasonable result.\n",
    "\n",
    "So, what kind of regularization should we try first? As we discussed in lesson 3, we should start with data augmentation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Often, to find the best data augmentation parameters, we need to try each type, one at time. \n",
    "\n",
    "And for each type, we can try 4 very different levels of augmentation, and see which one is the best.  In the steps below we've only kept the single best result we found. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Width shift: move the image left and right - "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1500 images belonging to 10 classes.\n"
     ]
    }
   ],
   "source": [
    "gen_t = image.ImageDataGenerator(width_shift_range=0.1)\n",
    "batches = get_batches(path+'train', gen_t, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "1500/1500 [==============================] - 32s - loss: 2.1572 - acc: 0.3293 - val_loss: 2.7141 - val_acc: 0.1950\n",
      "Epoch 2/2\n",
      "1500/1500 [==============================] - 25s - loss: 1.2215 - acc: 0.6333 - val_loss: 1.9040 - val_acc: 0.3660\n",
      "Epoch 1/4\n",
      "1500/1500 [==============================] - 36s - loss: 0.8126 - acc: 0.7580 - val_loss: 1.9323 - val_acc: 0.3180\n",
      "Epoch 2/4\n",
      "1500/1500 [==============================] - 30s - loss: 0.6064 - acc: 0.8280 - val_loss: 4.1675 - val_acc: 0.2040\n",
      "Epoch 3/4\n",
      "1500/1500 [==============================] - 24s - loss: 0.4107 - acc: 0.8893 - val_loss: 7.3811 - val_acc: 0.1540\n",
      "Epoch 4/4\n",
      "1500/1500 [==============================] - 25s - loss: 0.3534 - acc: 0.9073 - val_loss: 10.0822 - val_acc: 0.1550\n"
     ]
    }
   ],
   "source": [
    "model = conv1(batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1500 images belonging to 10 classes.\n",
      "Epoch 1/2\n",
      "1500/1500 [==============================] - 33s - loss: 2.7401 - acc: 0.1673 - val_loss: 6.5864 - val_acc: 0.1330\n",
      "Epoch 2/2\n",
      "1500/1500 [==============================] - 24s - loss: 2.1378 - acc: 0.2727 - val_loss: 4.5555 - val_acc: 0.1810\n",
      "Epoch 1/4\n",
      "1500/1500 [==============================] - 33s - loss: 1.9101 - acc: 0.3660 - val_loss: 2.8872 - val_acc: 0.1570\n",
      "Epoch 2/4\n",
      "1500/1500 [==============================] - 25s - loss: 1.7085 - acc: 0.4460 - val_loss: 2.7262 - val_acc: 0.2110\n",
      "Epoch 3/4\n",
      "1500/1500 [==============================] - 26s - loss: 1.5522 - acc: 0.4833 - val_loss: 3.6229 - val_acc: 0.1410\n",
      "Epoch 4/4\n",
      "1500/1500 [==============================] - 26s - loss: 1.4203 - acc: 0.5327 - val_loss: 10.0113 - val_acc: 0.1100\n"
     ]
    }
   ],
   "source": [
    "# Try a different level:\n",
    "gen_t = image.ImageDataGenerator(width_shift_range=0.5)\n",
    "batches = get_batches(path+'train', gen_t, batch_size=batch_size)\n",
    "model = conv1(batches)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are many other types that we can try. \n",
    "\n",
    "Here, let's save some time and put them together :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1500 images belonging to 10 classes.\n",
      "Epoch 1/2\n",
      "1500/1500 [==============================] - 33s - loss: 2.3887 - acc: 0.2400 - val_loss: 4.5639 - val_acc: 0.1980\n",
      "Epoch 2/2\n",
      "1500/1500 [==============================] - 25s - loss: 1.6648 - acc: 0.4513 - val_loss: 2.7483 - val_acc: 0.2190\n",
      "Epoch 1/4\n",
      "1500/1500 [==============================] - 35s - loss: 1.4291 - acc: 0.5160 - val_loss: 2.2258 - val_acc: 0.2660\n",
      "Epoch 2/4\n",
      "1500/1500 [==============================] - 26s - loss: 1.2220 - acc: 0.5980 - val_loss: 2.0314 - val_acc: 0.2810\n",
      "Epoch 3/4\n",
      "1500/1500 [==============================] - 26s - loss: 1.1074 - acc: 0.6433 - val_loss: 2.7005 - val_acc: 0.2600\n",
      "Epoch 4/4\n",
      "1500/1500 [==============================] - 25s - loss: 0.9478 - acc: 0.7060 - val_loss: 2.4790 - val_acc: 0.2210\n"
     ]
    }
   ],
   "source": [
    "gen_t = image.ImageDataGenerator(rotation_range=15, height_shift_range=0.05, \n",
    "                shear_range=0.1, channel_shift_range=20, width_shift_range=0.1)\n",
    "batches = get_batches(path+'train', gen_t, batch_size=batch_size)\n",
    "model = conv1(batches)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At first glance, this isn't looking encouraging, since the validation set is poor and getting worse. But the training set is getting better, and still has a long way to go in accuracy - so we should try annealing our learning rate and running more epochs, before we make a decisions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1500/1500 [==============================] - 33s - loss: 0.8260 - acc: 0.7413 - val_loss: 3.3064 - val_acc: 0.2110\n",
      "Epoch 2/5\n",
      "1500/1500 [==============================] - 25s - loss: 0.8478 - acc: 0.7260 - val_loss: 3.3521 - val_acc: 0.1630\n",
      "Epoch 3/5\n",
      "1500/1500 [==============================] - 27s - loss: 0.7473 - acc: 0.7560 - val_loss: 4.1624 - val_acc: 0.1980\n",
      "Epoch 4/5\n",
      "1500/1500 [==============================] - 26s - loss: 0.6712 - acc: 0.7873 - val_loss: 2.6913 - val_acc: 0.2350\n",
      "Epoch 5/5\n",
      "1500/1500 [==============================] - 26s - loss: 0.6504 - acc: 0.7993 - val_loss: 2.3551 - val_acc: 0.3160\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f5ddbe72278>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.optimizer.lr = 0.0001\n",
    "model.fit_generator(batches, batches.nb_sample, nb_epoch=5, validation_data=val_batches, \n",
    "                 nb_val_samples=val_batches.nb_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lucky we tried that - we starting to make progress! Let's keep going."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "1500/1500 [==============================] - 33s - loss: 0.5882 - acc: 0.8173 - val_loss: 2.2266 - val_acc: 0.3240\n",
      "Epoch 2/25\n",
      "1500/1500 [==============================] - 25s - loss: 0.5156 - acc: 0.8413 - val_loss: 1.8544 - val_acc: 0.3570\n",
      "Epoch 3/25\n",
      "1500/1500 [==============================] - 25s - loss: 0.4701 - acc: 0.8493 - val_loss: 1.6511 - val_acc: 0.3860\n",
      "Epoch 4/25\n",
      "1500/1500 [==============================] - 25s - loss: 0.4363 - acc: 0.8620 - val_loss: 1.4309 - val_acc: 0.4650\n",
      "Epoch 5/25\n",
      "1500/1500 [==============================] - 28s - loss: 0.4512 - acc: 0.8673 - val_loss: 1.2533 - val_acc: 0.5680\n",
      "Epoch 6/25\n",
      "1500/1500 [==============================] - 25s - loss: 0.4127 - acc: 0.8760 - val_loss: 0.8589 - val_acc: 0.6700\n",
      "Epoch 7/25\n",
      "1500/1500 [==============================] - 25s - loss: 0.3548 - acc: 0.8920 - val_loss: 0.5642 - val_acc: 0.8100\n",
      "Epoch 8/25\n",
      "1500/1500 [==============================] - 25s - loss: 0.3340 - acc: 0.9013 - val_loss: 0.5808 - val_acc: 0.7970\n",
      "Epoch 9/25\n",
      "1500/1500 [==============================] - 26s - loss: 0.3178 - acc: 0.9107 - val_loss: 0.5591 - val_acc: 0.8200\n",
      "Epoch 10/25\n",
      "1500/1500 [==============================] - 29s - loss: 0.3170 - acc: 0.9047 - val_loss: 0.7229 - val_acc: 0.7410\n",
      "Epoch 11/25\n",
      "1500/1500 [==============================] - 25s - loss: 0.3012 - acc: 0.9073 - val_loss: 0.3774 - val_acc: 0.8930\n",
      "Epoch 12/25\n",
      "1500/1500 [==============================] - 25s - loss: 0.2888 - acc: 0.9173 - val_loss: 0.4326 - val_acc: 0.8670\n",
      "Epoch 13/25\n",
      "1500/1500 [==============================] - 26s - loss: 0.2789 - acc: 0.9147 - val_loss: 0.2632 - val_acc: 0.9270\n",
      "Epoch 14/25\n",
      "1500/1500 [==============================] - 26s - loss: 0.2831 - acc: 0.9113 - val_loss: 0.4195 - val_acc: 0.8590\n",
      "Epoch 15/25\n",
      "1500/1500 [==============================] - 26s - loss: 0.2488 - acc: 0.9260 - val_loss: 0.3485 - val_acc: 0.8900\n",
      "Epoch 16/25\n",
      "1500/1500 [==============================] - 26s - loss: 0.2426 - acc: 0.9327 - val_loss: 0.6070 - val_acc: 0.7960\n",
      "Epoch 17/25\n",
      "1500/1500 [==============================] - 25s - loss: 0.2546 - acc: 0.9240 - val_loss: 0.5311 - val_acc: 0.8190\n",
      "Epoch 18/25\n",
      "1500/1500 [==============================] - 26s - loss: 0.2622 - acc: 0.9207 - val_loss: 0.3521 - val_acc: 0.8910\n",
      "Epoch 19/25\n",
      "1500/1500 [==============================] - 27s - loss: 0.2587 - acc: 0.9260 - val_loss: 0.2459 - val_acc: 0.9180\n",
      "Epoch 20/25\n",
      "1500/1500 [==============================] - 30s - loss: 0.2309 - acc: 0.9267 - val_loss: 0.2587 - val_acc: 0.9190\n",
      "Epoch 21/25\n",
      "1500/1500 [==============================] - 26s - loss: 0.2095 - acc: 0.9327 - val_loss: 0.2613 - val_acc: 0.9180\n",
      "Epoch 22/25\n",
      "1500/1500 [==============================] - 25s - loss: 0.2029 - acc: 0.9307 - val_loss: 0.2308 - val_acc: 0.9310\n",
      "Epoch 23/25\n",
      "1500/1500 [==============================] - 27s - loss: 0.1949 - acc: 0.9387 - val_loss: 0.2980 - val_acc: 0.8930\n",
      "Epoch 24/25\n",
      "1500/1500 [==============================] - 25s - loss: 0.2042 - acc: 0.9400 - val_loss: 0.2478 - val_acc: 0.9210\n",
      "Epoch 25/25\n",
      "1500/1500 [==============================] - 26s - loss: 0.1566 - acc: 0.9573 - val_loss: 0.2473 - val_acc: 0.9330\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f5ddbe72a58>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit_generator(batches, batches.nb_sample, nb_epoch=25, validation_data=val_batches, \n",
    "                 nb_val_samples=val_batches.nb_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Amazingly, using nothing but a small sample, a simple (not pre-trained) model with no dropout, and data augmentation, we're getting results that would get us into the top 50% of the competition! This looks like a great foundation for our futher experiments.\n",
    "To go further, we'll need to use the whole dataset, since dropout and data volumes are very related, so we can't tweak dropout without using all the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:py36]",
   "language": "python",
   "name": "conda-env-py36-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
